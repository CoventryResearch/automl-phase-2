\documentclass{article} % For LaTeX2e
\usepackage{format/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{amsmath, amsfonts, bm, lipsum, capt-of}
\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}
\DeclareCaptionType{copyrightbox}
\usepackage{float}

\usepackage{include/picins}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=none, rectangle]

\renewcommand{\baselinestretch}{1}

\title{Sensible allocation of computation\\for ensemble construction}

\author{
James Robert Lloyd\\
Department of Engineering\\
University of Cambridge\\
\And
Zoubin Ghahramani\\
Department of Engineering\\
University of Cambridge\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!

%\nipsfinalcopy

\begin{document}

\def\ParamSpace{\Theta}
\def\Param{\theta}
\def\Observation{y}
\def\ObservationVector{Y}
\def\ObservationSpace{\mathcal{Y}}
\def\Func{f}
\def\FuncTime{g}
\def\Noise{\varepsilon}
\def\Input{x}
\def\InputVector{X}
\def\InputSpace{\mathcal{X}}

\maketitle

%\vspace{-\baselineskip}

\begin{abstract} 
This paper describes the methods implemented in the [redacted] place entry to the \nth{2} round of the AutoML challenge, auto-track.
The methods are most succinctly described as an extension of freeze-thaw Bayesian optimization to ensemble construction.
In particular, we assume that we have access to a finite number of iterative learning algorithms and some method of forming an ensemble prediction from several predictions.
We then consider the decision process of which iterative algorithms to run and for how long in order to maximise the performance of the ensembled predictions at the end of some time limit.
Our approach to this Markov decision process is to model the reward function in a Bayesian fashion and then follow a mostly myopic strategy.
We also discuss some technical details such as managing memory usage and asynchrony.
Finally we position this work within the broader goal of automating as much of the machine learning pipeline as possible.
\end{abstract} 

\allowdisplaybreaks

%\vspace{-\baselineskip}

\section{Introduction}

This paper describes a method for automatic machine learning (AutoML); in particular it targets the construction of a predictive model.
At the time of writing a canonical way for an AutoML system to work would be to treat learning algorithms as black boxes that return predictions and an estimate of their performance.
The task is then to choose an algorithm and hyperparameters that maximise this estimate of performance (perhaps accounting for the noisy character of the estimates).

Variations on this canonical procedure include algorithms that exploit information from the learning curve of an iterative algorithm\fTBD{cite} and those that form an ensemble\footnotemark{} of base predictions\fTBD{cite}.
This paper describes a method that does both; it uses learning curve information to quickly determine which base learning algorithms will most contribute to the performance of an ensemble of predictions.

\footnotetext{This usually means an average but can refer to any method of combining several sets of predictions into one.}

An implementation of this method came [redacted] in the \nth{2} round of the AutoML challenge, auto-track.
For this competition, program robustness and other implementation details were important to ensure success; they are discussed as well as the method.

\section{Background}

\subsection{Model-based and Bayesian optimisation}

Consider the task of maximising some function ${\Func : \ParamSpace \to \Reals}$.
Suppose we observe evaluations of the function, potentially corrupted by noise, ${\Observation_i = \Func(\Param_i) + \Noise_i}$.
Model-based optimisation uses the observations ${\{(\Param_i, \Observation_i)\}_{i=1,\dots,N}}$ to draw inferences about $f$ and then uses this information to decide where to evaluate the (noisy) function next.
Model-based optimisation has recently gained popularity in the domain of hyperparameter optimisation for learning / statistical algorithms \ie optimisation of parameters not explicitly set by the learning / inference procedure.
Here, the noisy evaluations of the function $\Func$ are some estimate of performance of a learning algorithm \eg cross-validated performance under some metric.
Both frequentist\fTBD{cite} and Bayesian\fTBD{cite} approaches to inference have been explored in this domain and variations of all sorts (modelling assumptions, evaluation location selection strategy, exploitation of parallelism, etc.) abound\fTBD{cite}.

The main benefit of model-based optimisation in the context of hyper-parameter optimisation is the possibility of data efficiency.
That is, by introducing assumptions about the nature of function $\Func$ one can draw inferences about it after observing relatively few evaluations of the function compared to classical optimisation algorithms\footnotemark{}\fTBD{cite}.
When the function being optimised is the performance of a slow to evaluate learning algorithm, data efficiency is paramount, hence the focus on Bayesian inference methods and advanced evaluation location selection strategies\fTBD{cite}.

\footnotetext{Some classical optimisation algorithms have recently been interpreted as model-based optimisation using (approximate) inference.}

\subsection{Freeze-thaw Bayesian optimisation}

The importance of data efficiency, and thus time efficiency, is the motivation of freeze-thaw Bayesian optimisation\fTBD{cite} (FTBO).
This algorithm augments Bayesian optimisation for hyperparameter learning by using information from partially trained algorithms.
Concretely, assume that if we run a learning algorithm for time $t$ then we can obtain a noisy evaluation of its performance ${\Observation_{it} = \FuncTime(\Param_i, t) + \Noise_{it}}$.
FTBO assumes that algorithms can be paused and restarted, and constructs a decision procedure based on inferences of $\FuncTime$ to optimise algorithm performance at convergence \ie ${\Func(\Param_i) \defas \FuncTime(\Param_i, \infty)}$.

The central idea of this work is to use information from partially trained algorithms to prioritise which algorithms to run.
This idea has been explored previously under the title of racing algorithms\fTBD{cite} and there are contemporary related methods in the badits literature\fTBD{cite}.

\subsection{Stacking: a method of ensemble construction}

Combining multiple predictions into a single ensembled prediction almost always results in better performance; it is very rare for the winning solution of a data mining (or more accurately, prediciton) competition to not be an ensemble of different techniques\fTBD{cite}.
There are many methods for forming ensembles\fTBD{cite}, but we restrict attention to stacking\fTBD{cite} for simplicity.

Suppose we are learning a function ${\InputSpace \to \ObservationSpace}$ from data ${\InputVector, \ObservationVector = (\Input_i)_{i=1,\dots,N}, (\Observation_i)_{i=1,\dots,N}}$.
Denote the predictions\footnotemark{} from learning algorithm $j$ by $\hat\ObservationVector^j$.
Stacking forms an ensemble of several predictions be learning a function ${(\hat\Observation_i^1,\dots,\hat\Observation_i^J) \to \Observation_i}$ for all $i$.
That is, stacking first concatenates the outputs from the base learning algorithms and then learns a function from these concatenated outputs to the output value of the original data.

\footnotetext{In \eg a classification problem the prediction might be either a class label or a probability value / vector.}

As described above it may appear that the training data is being used twice, which might result in some form of overconfidence.
This can be remedied easily by using two sets of validation data or by cross validation which is the approach taken here.

\section{Description of method}
\label{sec:method}

In this section we give descriptions with increasing levels of detail of the algorithm we used in the \nth{2} round of the AutoML challenge, auto-track.
The code we submitted to the competition is available on github\footnotemark{}.
Since this code was written under time pressure, many details of the algorithm were chosen somewhat arbitrarily or by rules of thumb; we discuss these choices at the end of this manuscript.

\footnotetext{[Redacted]}

\subsection{The main ideas}

The main idea behind this method is to try to predict how much the performance of an ensembled prediction will improve if a base algorithm is run for a certain amount of extra time.
Contrast this with FTBO that predicts how much an individual algorithm will improve; this is the small but important innovation.
The main design principles of implementation are those of modularity, parallelism and fault tolerance so that any part of the algorithm can fail or become slow and the whole algorithm will still do something sensible.

\subsection{Description}

We assume we have access to a finite number of base learning algorithms with fixed hyperparameters; some of these algorithms are iterative.
The training data is split into five equally sized folds and each algorithm is run on the five folds in parallel, producing predictions for the held out data and five sets of predictions for the validation and testing data, which are then averaged.
The predictions on the held out data are used to estimate the performance of the base learning algorithms under some metric.

The non-iterative algorithms are run in sequence, and are terminated after producing predictions or after a time limit.
The iterative algorithms are run for a set period of time in sequence and are then paused, but are terminated if they do not produce predictions after a time limit.

Whenever new predictions are made by a base learner or an iterative algorithm updates its predictions, the stacking algorithm is run.
The inputs for the stacking learning problem are the concatenated predicted class probabilities from each base learner on the held out data.
To learn the stacking function, logistic regression is used, with constraints on the coefficients as follows.
Let $c_{ijk}$ be the coefficient for base algorithm $i$, input class label $j$ and output class label $k$.
Then,
\begin{align}
  c_{ijk} &= 0\phantom{c_{ij'k'}} \quad \textrm{if}\,\, j \neq k \\
  c_{ijk} &= c_{ij'k'}\phantom{0} \quad \forall\,\, j, j', k, k'
\end{align}
i.e. the ensemble prediction is a weighted vector sum of base probability predictions followed by a softmax.
The performance of stacking is estimated via five fold cross validation\footnotemark{}.

\footnotetext{These folds need not be related to the orignal folds.}

The changes in performance of each iterative base learner are recorded along with the corresponding change to the performance of the ensemble.
Together with features recording other aspects of the state of base learners and the stacked ensemble a decision tree learning algorithm is used to learn a function to map between improvements in individual learners and improvements in the ensemble.
Similar methods to those proposed in FTBO are used to extrapolate individual learning curves which allows for one to estimate how much the ensemble performance will improve if a single learning algorithm is run for a certain amount of extra time.

For each iterative learning algorithm, we calculate the expected improvement to the performance of the ensemble prediction if the algorithm is run for a time equal to an integer multiple of some base quantum of time.
These expected improvements are discounted through time with a constant discount factor.
The algorithm with the highest discounted expected improvement at some time is then chosen as the next algorithm to run.

This procedure iterates (asynchronously) until the time limit is reached.

\subsection{Details of the method}

\paragraph{Base learning algorithms}

We used the scikit-learn implementations of the following non-iterative\footnotemark{} algorithms
\begin{itemize}
  \item Logistic regression with $\ell_1$ regularisation
  \item Logistic regression with $\ell_2$ regularisation
  \item k-nearest neighbours
  \item Gaussian naive Bayes
  \item Decision trees
\end{itemize}
\footnotetext{Many can be implemented iteratively.}
and the following iterative algorithms
\begin{itemize}
  \item Random forest\footnote{A small modification is required to make this an iterative algorithm using the scikit-learn implementation.}
  \item Gradient boosted decision trees
\end{itemize}
Each algorithm was run with around 3 or 5 different parameter settings (see code for exact values).
In addition to different parameter values, base learners were constructed that only ran on a subset of data points and/or features.
This was a simple guard against a very large dataset preventing anythng from running due to memory constraints.

\paragraph{Cross validation}

The training data was split into 5 equal folds.
In parallel for each folds, each algorithm was trained on the 4 other folds, producing predictions for the held-out fold and the validation and testing data.
The held-out predictions were concatenated resulting in held-out predictions from each algorithm for the entire training data.
The 5 predictions for the validation and testing data were averaged to produce a single set of predictions from the learning algorithm.

\paragraph{Extrapolating learning curves}

The iterative algorithms estimated their performance under an approrpriate metric using the held out predictions 10 times per quantum of time (about 15 seconds).
These estimates of performance were then extrapolated using Gaussian process regression using the kernel intoduced in FTBO\fTBD{cite} and the same inference methods \ie slice sampling\fTBD{cite} of hyperparameters and analytic integration of the Gaussian process.
For simplicity, each algorithm's learning curve was modelled independently of the rest.
Extrapolations were represented as samples from the posterior distribution.

\paragraph{Predicting improvements to the ensembled prediction}

Each time the stacking algorithm was run after an iterative algorithm was updated, the following data was recorded
\begin{itemize}
  \item $\Delta_\textrm{best}$ - the performance of the individual algorithm minus the previous individual base learner best performance
  \item $\Delta_\textrm{ind}$ - the performance increase of the individual algorithm
  \item $\Delta_\textrm{ens}$ - the performance increase of the ensemble
\end{itemize}
$\Delta_\textrm{ens}$ was then modelled as
\[
  \Delta_\textrm{ens} = \alpha(\Delta_\textrm{best})\Delta_\textrm{ind}
\]
where the function $\alpha$ was learnt using a decision tree learning algorithm.

\paragraph{Missing data and feature selection}

We used the same data loading and preprocessing routines provided by the AutoML challenge organisers\fTBD{cite}.

\subsection{Details of implementation}

\paragraph{Parallelism and message passing}

The most important details of the implementation are parallelism and memory management.
The following processes ran asynchronously in parallel using message passing to communicate information
\begin{itemize}
  \item The main thread, which would terminate everything if memory usage was dangerously high or the time limit was approaching
  \item The manager, which was responsible for creating, pausing and terminating all processes as well as instructing processes to serialise to disk
  \item The learning curve extrapolator
  \item The stacker, which also produced estimates of how much each algorithm was expected to improve the performance of the ensemble if run for more time
  \item For each base learning algorithm
  \begin{itemize}
    \item A cross validator which would combine and evaluate the predictions of\dots
    \item \dots five workers which would produce predictions
  \end{itemize}
\end{itemize}

Training data was always loaded into memory but shared between processes wherever possible.
The manager would use the expected performance improvements calculated by the stacker to rank all of the base learning algorithms.
When memory usage became low the manager would instruct the lowest ranked processes to serialise to disk.
If memory usage became very low the manager would pause all programs until memory usage reduced.
If a process took too long to serialise to disk then it would be terminated.

\section{Empirical performance}

Our evaluation of the method described in this paper is currently limited to the competition performance where we were placed [redacted] in the \nth{2} round of the AutoML challenge, auto-track.
The performance of the top five participants is shown below

\begin{table}
  \center
  \begin{tabular}{lrrrrrr}
    Team name & Average rank & P1 & P2 & P3 & P4 & P5 \\
    backstreet.bayes & 1.8 & 0.3193 (3) & x & x & x & x\\
    aad\_freiburg    & 3.4 & DNF (last) & x & x & x & x \\
    lukasz.romano    & 3.8 & 0.3304 (2) & x & x & x & x \\
    matthias.vonrohr & 4.4 & 0.3029 (5) & x & x & x & x \\
    marc.boulle      & 4.4 & 0.3533(1) & x & x & x & x
  \end{tabular}
\end{table}

\section{Discussion}

Discussion

\subsection{What is right with this method?}

Ensembling and paying keen attention to time.

\subsection{What is wrong with this method?}

Cross validation.

Hyperparameter optimisation.

Ensembling as a separate module - rather than boosting or something similar.

\subsection{What have we not talked about?}

This is not bounded rational - it's kinda like meta reasoning so certainly not optimal in the long run.

Machine learning is a pipe-line - pre and post processing clearly missing.

Off-line learning.

Proper data subsetting strategies for algorithms not easily made stochastic.

An ensemble is just a prediction method - why the distinction?

%\newpage

\small

\bibliography{library}
\bibliographystyle{unsrt}

\end{document} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%