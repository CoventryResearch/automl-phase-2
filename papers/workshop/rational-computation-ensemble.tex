\documentclass{article} % For LaTeX2e
\usepackage{format/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{amsmath, amsfonts, bm, lipsum, capt-of}
\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}
\DeclareCaptionType{copyrightbox}
\usepackage{float}

\usepackage{include/picins}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=none, rectangle]

\renewcommand{\baselinestretch}{1}

\title{Sensible allocation of computation\\for ensemble construction}

\author{
James Robert Lloyd\\
Department of Engineering\\
University of Cambridge\\
\And
Zoubin Ghahramani\\
Department of Engineering\\
University of Cambridge\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!

%\nipsfinalcopy

\begin{document}

\def\ParamSpace{\Theta}
\def\Param{\theta}
\def\Observation{y}
\def\ObservationVector{Y}
\def\ObservationSpace{\mathcal{Y}}
\def\Func{f}
\def\FuncTime{g}
\def\Noise{\varepsilon}
\def\Input{x}
\def\InputVector{X}
\def\InputSpace{\mathcal{X}}

\maketitle

%\vspace{-\baselineskip}

\begin{abstract} 
This paper describes the methods implemented in the [redacted] place entry to the \nth{2} round of the AutoML challenge, auto-track.
The methods are most succinctly described as an extension of freeze-thaw Bayesian optimization to ensemble construction.
In particular, we assume that we have access to a finite number of iterative learning algorithms and some method of forming an ensemble prediction from several predictions.
We then consider the decision process of which iterative algorithms to run and for how long in order to maximise the performance of the ensembled predictions at the end of some time limit.
Our approach to this Markov decision process is to model the reward function in a Bayesian fashion and then follow a mostly myopic strategy.
We also discuss some technical details such as managing memory usage and asynchrony.
Finally we position this work within the broader goal of automating as much of the machine learning pipeline as possible.
\end{abstract} 

\allowdisplaybreaks

%\vspace{-\baselineskip}

\section{Introduction}

Intro

\section{Background}

\subsection{Model-based and Bayesian optimisation}

Consider the task of maximising some function $\Func : \ParamSpace \to \Reals$.
Suppose we observe evaluations of the function, potentially corrupted by noise, $\Observation_i = \Func(\Param_i) + \Noise_i$.
Model-based optimisation uses the observations $\{(\Param_i, \Observation_i)\}_{i=1,\dots,N}$ to draw inferences about $f$ and then uses this information to decide where to evaluate the (noisy) function next.
Model-based optimisation has gained popularity in the domain of hyperparameter optimisation for learning / statistical algorithms \ie optimisation of parameters not explicitly set by the learning / inference procedure.
Here, the noisy evaluations of the function $\Func$ are some estimate of performance of a learning algorithm \eg cross-validated performance under some metric.
Both frequentist\fTBD{cite} and Bayesian\fTBD{cite} approaches to inference have been explored in this domain and many variations of all sorts (modelling assumptions, evaluation location selection strategy, exploitation of parallelism, et cetera) abound\fTBD{cite}.

The main benefit of model-based optimisation in the context of hyper-parameter optimisation is the possibility of data efficiency.
That is, by introducing assumptions about the nature of function $\Func$ one can draw inferences about it after observing relatively few evaluations of the function compared to classical optimisation algorithms.
When the function being optimised is the performance of a slow to evaluate learning algorithm, data efficiency is paramount, hence the focus on Bayesian inference methods and evaluation location selection strategies\fTBD{cite}.

\subsection{Freeze-thaw Bayesian optimisation}

The importance of data efficiency, and thus time efficiency, is the motivation of freeze-thaw Bayesian optimisation\fTBD{cite} (FTBO).
This algorithm augments Bayesian optimisation for hyperparameter learning by using information from partially trained algorithms.
Concretely, assume that if we run a learning algorithm for time $t$ then we can obtain a noisy evaluation of its performance $\Observation_{it} = \FuncTime(\Param_i, t) + \Noise_{it}$.
FTBO assumes that algorithms can be paused and restarted, and constructs a decision procedure based on inferences of $\FuncTime$ to optimise algorithm performance at convergence \ie $\Func(\Param_i) \defas \FuncTime(\Param_i, \infty)$.

The central idea of this work is to use information from partially trained algorithms to prioritise which algorithms to run.
This idea has been explored previously under the title of racing algorithms\fTBD{cite} and there are contemporary related methods in the badits literature\fTBD{cite}.

\subsection{Stacking: a method of ensemble construction}

Combining multiple predictions into a single ensembled prediction almost always results in better performance; it is very rare for the winning solution of a data mining (or more accurately, prediciton) competition to not be an ensemble of different techniques\fTBD{cite}.
There are many methods for forming ensembles\fTBD{cite}, but we restrict attention to stacking\fTBD{cite} for simplicity.

Suppose we are learning a function $\InputSpace \to \ObservationSpace$ from data $\InputVector, \ObservationVector = (\Input_i)_{i=1,\dots,N}, (\Observation_i)_{i=1,\dots,N}$.
Denote the prediction\footnotemark{} from learning algorithm $j$ by $\hat\ObservationVector^j$.
Stacking forms an ensemble of several predictions be learning a function $(\hat\Observation_i^1,\dots,\hat\Observation_i^J) \to \Observation_i$ for all $i$.
That is, stacking first concatenates the outputs from the base learning algorithms and then learns a function from these concatenated outputs to the output value of the original data.

\footnotetext{In \eg a classification problem the prediction might be either a class label or a probability value / vector.}

As described above it may appear that the training data is being used twice, which might results in some form of overconfidence.
This can be remedied easily by using two sets of validation data or by cross validation (which we describe in section~\ref{sec:method}).

\section{Description of method}
\label{sec:method}

In this section we give descriptions with increasing levels of detail of the algorithm we used in the \nth{2} round of the AutoML challenge, auto-track.
The code we submitted to the competition is available on github\footnotemark{}.
Many details of the algorithm were chosen somewhat arbitrarily or by rules of thumb; we discuss these choices at the end of this manuscript.

\footnotetext{[Redacted]}

\subsection{The main ideas}

The main idea behind this method is to try to predict how much the performance of an ensembled prediction will improve if a base algorithm is run for a certain amount of extra time.
Contrast this with FTBO that predicts how much an individual algorithm will improve.
The main design principles of implementation are those of modularity, parallelism and fault tolerance so that any part of the algorithm can fail or become slow and the whole algorithm will still do something sensible.

\subsection{Description}

We assume we have access to a finite number of base learning algorithms with fixed hyperparameters; some of these algorithms are iterative.
The training data is split into five equally sized folds and each algorithm is run on the five folds in parallel, producing predictions for the held out data and five sets of predictions for the validation and testing data, which are then averaged.
The predictions on the held out data are used to estimate the performance of the base learning algorithms under some metric.

The non-iterative algorithms are run in sequence, and are terminated after producing predictions or after a time limit.
The iterative algorithms are run for a set period of time in sequence and are then paused, but are terminated is they do not produce predictions after a time limit.

Whenever new predictions are made by a base learner or an iterative algorithm updates its predictions, the stacking algorithm is run.
The inputs for the stacking learning problem are the concatenated predicted class probabilities from each base learner on the held out data.
To learn the stacking function, logistic regression is used, where the coefficients are constrained to be equal across input class labels for each base learner i.e. a weighted sum of base probability predictions followed by a softmax.
The performance of stacking is estimated via five fold cross validation.

The changes in performance of each iterative base learner are recorded along with the corresponding change to the performance of the ensemble.
Together with features recording other aspects of the state of base learners and the stacked ensemble a decision tree learning algorithm is used to learn a function to map between improvements in individual learners and the ensemble.
Similar methods to those proposed in FTBO are used to extrapolate individual learning curves which allows for one to estimate how much the ensemble performance will improve if a single learning algorithm is run for a certain amount of extra time.

For each iterative learning algorithm, the expected improvement to the performance of the ensemble prediction is caluclated is the algorithm is run for a time equal to an integer multiple of some base quantum of time.
These expected improvements are discounted through time with a constant discount factor.
The algorithm with the highest discounted expected improvement at some time is then chosen as the next algorithm to run.

\subsection{Details of the method}

FTBO kernel.

Decision tree features and prior.

Base learners (data and feature subsetting).

Held out data and averaging of stuff.

Details of parameters are unimportant (since they are mostly arbitrary choices) but can be found in the code.

Missing data and feature selection as per the sample code.

\subsection{Details of implementation}

Memory management including memory sharing and startegies for sparse data.

Message passing.

Parallelism.

\section{Empirical performance}

We were placed [redacted].
The results are below.

[TABLE]

\section{Discussion}

Discussion

\subsection{What is right with this method?}

Ensembling and paying keen attention to time.

\subsection{What is wrong with this method?}

Cross validation.

Hyperparameter optimisation.

Ensembling as a separate module - rather than boosting or something similar.

\subsection{What have we not talked about?}

This is not bounded rational - it's kinda like meta reasoning so certainly not optimal in the long run.

Machine learning is a pipe-line - pre and post processing clearly missing.

Off-line learning.

%\newpage

\small

\bibliography{library}
\bibliographystyle{unsrt}

\end{document} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%