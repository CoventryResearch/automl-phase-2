\documentclass{article} % For LaTeX2e
\usepackage{format/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{amsmath, amsfonts, bm, lipsum, capt-of}
\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}
\DeclareCaptionType{copyrightbox}
\usepackage{float}

\usepackage{include/picins}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=none, rectangle]

\renewcommand{\baselinestretch}{1}

\title{Sensible allocation of computation\\for ensemble construction}

\author{
James Robert Lloyd\\
Department of Engineering\\
University of Cambridge\\
\And
Zoubin Ghahramani\\
Department of Engineering\\
University of Cambridge\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!

%\nipsfinalcopy

\begin{document}

\def\ParamSpace{\Theta}
\def\Param{\theta}
\def\Observation{y}
\def\ObservationVector{Y}
\def\ObservationSpace{\mathcal{Y}}
\def\Func{f}
\def\FuncTime{g}
\def\Noise{\varepsilon}
\def\Input{x}
\def\InputVector{X}
\def\InputSpace{\mathcal{X}}

\maketitle

%\vspace{-\baselineskip}

\begin{abstract} 
This paper describes the methods implemented in the [redacted] place entry to the \nth{2} round of the AutoML challenge, auto-track.
The methods are most succinctly described as an extension of freeze-thaw Bayesian optimization to ensemble construction.
In particular, we assume that we have access to a finite number of iterative learning algorithms and some method of forming an ensemble prediction from several predictions.
We then consider the decision process of which iterative algorithms to run and for how long in order to maximise the performance of the ensembled predictions at the end of some time limit.
Our approach to this Markov decision process is to model the reward function in a Bayesian fashion and then follow a mostly myopic strategy.
We also discuss some technical details such as managing memory usage and asynchrony.
Finally we position this work within the broader goal of automating as much of the machine learning pipeline as possible.
\end{abstract} 

\allowdisplaybreaks

%\vspace{-\baselineskip}

\section{Introduction}

This paper describes a method for automatic machine learning (AutoML); in particular it targets the construction of a predictive model.
At the time of writing a canonical way for an AutoML system to work would be to treat learning algorithms as black boxes that return predictions and an estimate of their performance.
The task is then to choose an algorithm and hyperparameters that maximise this estimate of performance (perhaps accounting for the noisy character of the estimates).

Variations on this canonical procedure include algorithms that exploit information from the learning curve of an iterative algorithm\fTBD{cite} and those that form an ensemble\footnotemark{} of base predictions\fTBD{cite}.
This paper describes a method that does both; it uses learning curve information to quickly determine which base learning algorithms will most contribute to the performance of an ensemble of predictions.

\footnotetext{This usually means an average but can refer to any method of combining several sets of predictions into one.}

An implementation of this method came [redacted] in the \nth{2} round of the AutoML challenge, auto-track.
For this competition, program robustness and other implementation details were important to ensure success; they are discussed as well as the method.

\section{Background}

\subsection{Model-based and Bayesian optimisation}

Consider the task of maximising some function ${\Func : \ParamSpace \to \Reals}$.
Suppose we observe evaluations of the function, potentially corrupted by noise, ${\Observation_i = \Func(\Param_i) + \Noise_i}$.
Model-based optimisation uses the observations ${\{(\Param_i, \Observation_i)\}_{i=1,\dots,N}}$ to draw inferences about $f$ and then uses this information to decide where to evaluate the (noisy) function next.
Model-based optimisation has recently gained popularity in the domain of hyperparameter optimisation for learning / statistical algorithms \ie optimisation of parameters not explicitly set by the learning / inference procedure.
Here, the noisy evaluations of the function $\Func$ are some estimate of performance of a learning algorithm \eg cross-validated performance under some metric.
Both frequentist\fTBD{cite} and Bayesian\fTBD{cite} approaches to inference have been explored in this domain and variations of all sorts (modelling assumptions, evaluation location selection strategy, exploitation of parallelism, etc.) abound\fTBD{cite}.

The main benefit of model-based optimisation in the context of hyper-parameter optimisation is the possibility of data efficiency.
That is, by introducing assumptions about the nature of function $\Func$ one can draw inferences about it after observing relatively few evaluations of the function compared to classical optimisation algorithms\footnotemark{}\fTBD{cite}.
When the function being optimised is the performance of a slow to evaluate learning algorithm, data efficiency is paramount, hence the focus on Bayesian inference methods and advanced evaluation location selection strategies\fTBD{cite}.

\footnotetext{Some classical optimisation algorithms have recently been interpreted as model-based optimisation using (approximate) inference.}

\subsection{Freeze-thaw Bayesian optimisation}

The importance of data efficiency, and thus time efficiency, is the motivation of freeze-thaw Bayesian optimisation\fTBD{cite} (FTBO).
This algorithm augments Bayesian optimisation for hyperparameter learning by using information from partially trained algorithms.
Concretely, assume that if we run a learning algorithm for time $t$ then we can obtain a noisy evaluation of its performance ${\Observation_{it} = \FuncTime(\Param_i, t) + \Noise_{it}}$.
FTBO assumes that algorithms can be paused and restarted, and constructs a decision procedure based on inferences of $\FuncTime$ to optimise algorithm performance at convergence \ie ${\Func(\Param_i) \defas \FuncTime(\Param_i, \infty)}$.

The central idea of this work is to use information from partially trained algorithms to prioritise which algorithms to run.
This idea has been explored previously under the title of racing algorithms\fTBD{cite} and there are contemporary related methods in the badits literature\fTBD{cite}.

\subsection{Stacking: a method of ensemble construction}

Combining multiple predictions into a single ensembled prediction almost always results in better performance; it is very rare for the winning solution of a data mining (or more accurately, prediciton) competition to not be an ensemble of different techniques\fTBD{cite}.
There are many methods for forming ensembles\fTBD{cite}, but we restrict attention to stacking\fTBD{cite} for simplicity.

Suppose we are learning a function ${\InputSpace \to \ObservationSpace}$ from data ${\InputVector, \ObservationVector = (\Input_i)_{i=1,\dots,N}, (\Observation_i)_{i=1,\dots,N}}$.
Denote the predictions\footnotemark{} from learning algorithm $j$ by $\hat\ObservationVector^j$.
Stacking forms an ensemble of several predictions be learning a function ${(\hat\Observation_i^1,\dots,\hat\Observation_i^J) \to \Observation_i}$ for all $i$.
That is, stacking first concatenates the outputs from the base learning algorithms and then learns a function from these concatenated outputs to the output value of the original data.

\footnotetext{In \eg a classification problem the prediction might be either a class label or a probability value / vector.}

As described above it may appear that the training data is being used twice, which might result in some form of overconfidence.
This can be remedied easily by using two sets of validation data or by cross validation which is the approach taken here.

\section{Description of method}
\label{sec:method}

In this section we give descriptions with increasing levels of detail of the algorithm we used in the \nth{2} round of the AutoML challenge, auto-track.
The code we submitted to the competition is available on github\footnotemark{}.
Since this code was written under time pressure, many details of the algorithm were chosen somewhat arbitrarily or by rules of thumb; we discuss these choices at the end of this manuscript.

\footnotetext{[Redacted]}

\subsection{The main ideas}

The main idea behind this method is to try to predict how much the performance of an ensembled prediction will improve if a base algorithm is run for a certain amount of extra time.
Contrast this with FTBO that predicts how much an individual algorithm will improve; this is the small but important innovation.
The main design principles of implementation are those of modularity, parallelism and fault tolerance so that any part of the algorithm can fail or become slow and the whole algorithm will still do something sensible.

\subsection{Description}

We assume we have access to a finite number of base learning algorithms with fixed hyperparameters; some of these algorithms are iterative.
The training data is split into five equally sized folds and each algorithm is run on the five folds in parallel, producing predictions for the held out data and five sets of predictions for the validation and testing data, which are then averaged.
The predictions on the held out data are used to estimate the performance of the base learning algorithms under some metric.

The non-iterative algorithms are run in sequence, and are terminated after producing predictions or after a time limit.
The iterative algorithms are run for a set period of time in sequence and are then paused, but are terminated if they do not produce predictions after a time limit.

Whenever new predictions are made by a base learner or an iterative algorithm updates its predictions, the stacking algorithm is run.
The inputs for the stacking learning problem are the concatenated predicted class probabilities from each base learner on the held out data.
To learn the stacking function, logistic regression is used, with constraints on the coefficients as follows.
Let $c_{ijk}$ be the coefficient for base algorithm $i$, input class label $j$ and output class label $k$.
Then,
\begin{align}
  c_{ijk} &= 0\phantom{c_{ij'k'}} \quad \textrm{if}\,\, j \neq k \\
  c_{ijk} &= c_{ij'k'}\phantom{0} \quad \forall\,\, j, j', k, k'
\end{align}
i.e. the ensemble prediction is a weighted vector sum of base probability predictions followed by a softmax.
The performance of stacking is estimated via five fold cross validation\footnotemark{}.

\footnotetext{These folds need not be related to the orignal folds.}

The changes in performance of each iterative base learner are recorded along with the corresponding change to the performance of the ensemble.
Together with features recording other aspects of the state of base learners and the stacked ensemble a decision tree learning algorithm is used to learn a function to map between improvements in individual learners and improvements in the ensemble.
Similar methods to those proposed in FTBO are used to extrapolate individual learning curves which allows for one to estimate how much the ensemble performance will improve if a single learning algorithm is run for a certain amount of extra time.

For each iterative learning algorithm, we calculate the expected improvement to the performance of the ensemble prediction if the algorithm is run for a time equal to an integer multiple of some base quantum of time.
These expected improvements are discounted through time with a constant discount factor.
The algorithm with the highest discounted expected improvement at some time is then chosen as the next algorithm to run.

This procedure iterates (asynchronously) until the time limit is reached.

\subsection{Details of the method}

\paragraph{Base learning algorithms}

We used the scikit-learn implementations of the following non-iterative\footnotemark{} algorithms
\begin{itemize}
  \item Logistic regression with $\ell_1$ regularisation
  \item Logistic regression with $\ell_2$ regularisation
  \item k-nearest neighbours
  \item Gaussian naive Bayes
  \item Decision trees
\end{itemize}
\footnotetext{Many can be implemented iteratively.}
and the following iterative algorithms
\begin{itemize}
  \item Random forest\footnote{A small modification is required to make this an iterative algorithm using the scikit-learn implementation.}
  \item Gradient boosted decision trees
\end{itemize}
Each algorithm was run with around 3 or 5 different parameter settings (see code for exact values).
In addition to different parameter values, base learners were constructed that only ran on a subset of data points and/or features.
This was a simple guard against a very large dataset preventing anythng from running due to memory constraints.

\paragraph{Cross validation}

The training data was split into 5 equal folds.
In parallel for each folds, each algorithm was trained on the 4 other folds, producing predictions for the held-out fold and the validation and testing data.
The held-out predictions were concatenated resulting in held-out predictions from each algorithm for the entire training data.
The 5 predictions for the validation and testing data were averaged to produce a single set of predictions from the learning algorithm.

\paragraph{Extrapolating learning curves}

The iterative algorithms estimated their performance under an approrpriate metric using the held out predictions 10 times per quantum of time (about 15 seconds).
These estimates of performance were then extrapolated using Gaussian process regression using the kernel intoduced in FTBO\fTBD{cite} and the same inference methods \ie slice sampling\fTBD{cite} of hyperparameters and analytic integration of the Gaussian process.
For simplicity, each algorithm's learning curve was modelled independently of the rest.
Extrapolations were represented as samples from the posterior distribution.

\paragraph{Predicting improvements to the ensembled prediction}

Each time the stacking algorithm was run after an iterative algorithm was updated, the following data was recorded
\begin{itemize}
  \item $\Delta_\textrm{best}$ - the performance of the individual algorithm minus the previous individual base learner best performance
  \item $\Delta_\textrm{ind}$ - the performance increase of the individual algorithm
  \item $\Delta_\textrm{ens}$ - the performance increase of the ensemble
\end{itemize}
$\Delta_\textrm{ens}$ was then modelled as
\[
  \Delta_\textrm{ens} = \alpha(\Delta_\textrm{best})\Delta_\textrm{ind}
\]
where the function $\alpha$ was learnt using a decision tree learning algorithm.

\paragraph{Missing data and feature selection}

We used the same data loading and preprocessing routines provided by the AutoML challenge organisers\fTBD{cite}.

\subsection{Details of implementation}

\paragraph{Parallelism and message passing}

The most important details of the implementation are parallelism and memory management.
The following processes ran asynchronously in parallel using message passing to communicate information
\begin{itemize}
  \item The main thread, which would terminate everything if memory usage was dangerously high or the time limit was approaching
  \item The manager, which was responsible for creating, pausing and terminating all processes as well as instructing processes to serialise to disk
  \item The learning curve extrapolator
  \item The stacker, which also produced estimates of how much each algorithm was expected to improve the performance of the ensemble if run for more time
  \item For each base learning algorithm
  \begin{itemize}
    \item A cross validator which would combine and evaluate the predictions of\dots
    \item \dots five workers which would produce predictions
  \end{itemize}
\end{itemize}

\paragraph{Memory management}

Training data was always loaded into memory but shared between processes wherever possible.
The manager would use the expected performance improvements calculated by the stacker to rank all of the base learning algorithms.
When memory usage became low the manager would instruct the lowest ranked processes to serialise to disk.
If memory usage became very low the manager would pause all programs until memory usage reduced.
If a process took too long to serialise to disk then it would be terminated.

\section{Empirical performance}

Our evaluation of the method described in this paper is currently limited to the competition performance where we were placed [redacted] in the \nth{2} round of the AutoML challenge, auto-track.
For a detailed description of the competition see \fTBD{cite} and follow up papers published by the organisers.

The performance of the top five participants is shown in table~\ref{table:automlresults}.
The table lists the team name, mean rank over the five datasets, and the performance (under some metric) on each dataset together with the rank of this value.

\begin{table}[ht]
  \center
  \begin{tabular}{|l|r|r|r|r|r|r|}
    \hline
    Team name & Mean rank & Set 1 & Set 2 & Set 3 & Set 4 & Set 5 \\
    \hline
    backstreet.bayes & 1.8 & 0.3193 (3) & 0.9198 (1) & 0.3361 (1) & 0.3495 (2) & 0.2351 (2)\\
    \hline
    aad\_freiburg    & 3.4 & --- (last) & 0.8688 (2) & 0.3214 (2) & 0.3357 (4) & 0.2386 (1) \\
    \hline
    lukasz.romano    & 3.8 & 0.3304 (2) & 0.6662 (4) & 0.2647 (5) & 0.3440 (3) & 0.2194 (5) \\
    \hline
    matthias.vonrohr & 4.4 & 0.3029 (5) & 0.5939 (5) & 0.3064 (3) & 0.2994 (6) & 0.2220 (3) \\
    \hline
    marc.boulle      & 4.4 & 0.3533 (1) & 0.4561 (7) & 0.2130 (7) & 0.3692 (1) & 0.1434 (6) \\
    \hline
  \end{tabular}
  \caption[AutoML results]{AutoML challenge, round 2, auto-track, top 5 results}
  \label{table:automlresults}
\end{table}

\section{Discussion}

We now discuss what is good about this method, what can be improved, and situate this work within the broader goal of automating the machine learning pipeline.

\subsection{What is right with this method?}

\paragraph{Ensembling}

Much of the AutoML literature to date has focused on model selection.
However, ensembling can provide a sometimes substantial boost in performance for relatively little conceptual effort.
Figure\fTBD{insert figure} shows the estimated performance of many individual learning algorithms (left) and the estimated performance of an ensemble formed from them (right), both through time.
From early on the ensemble dominates all individual algorithms by a wide margin.

\paragraph{Quickly halting unpromising algorithms}

The search space of all machine learning algorithms is very large indeed.
In order to be able to seach this space quickly it is paramount to be able to halt unpromising lines of enquiry early, that is, it is important to fail fast.

\subsection{What is wrong with this method?}

\paragraph{Cross validation}

For large data, 5 fold cross validation is overkill; it would produce unnecessarily confident estimates of predictive performance.
For small data it will produce high variance estimates, increasing chances of overfitting when selecting which predictions to use (ensemble or individual algorithm and at what time).

\paragraph{Hyperparameter optimisation}

We made the simplifying assumption that there are only finitely many learning algorithms and parameter settings.
Clearly it might be advisable to search (continuous) parameter spaces more throroughly.

\paragraph{Modularity}

For conceptual simplicity, this system was designed in a modular fashion.
However, this is not a reflection of any theoretical modularity of the problem being solved.
In particular, ensembling need not be independent of base learning algorithms; methods where the ensembling algorithm and base learners interact (\eg boosting\fTBD{cite}) may yield better results.

\subsection{What have we not talked about?}

\paragraph{Optimising pipelines, not just learning algorithms}

Machine learning in the wild often involves data collection, data cleaning, preprocessing, postprocessing and many other processes other than modelling\fTBD{Cite Caruana?}.
In particular, preprocessing and postprocessing methods are absent from our method.
The challenge for incorporating such methods in an automated system is at least two fold.
First, the value of a preprocessing method may only be apparent after a learning algorithm has been run on its output; knowing how long to run an iterative preprocessing method may be difficult.
Second, including pre and postprocessing routines vastly increases the search space of potential learning pipelines, especially since such methods can be composed.

\paragraph{Off-line learning}

The current system starts each problem with very little knowledge about typical datasets or how well different algorithms are likely to perform.
Meta-learning\fTBD{cite} and reinforcement learning seem like worthwhile avenues to explore.

\paragraph{Bounded rationality}

The task of AutoML is to find the best algorithm with finite computational resources.
Limited resources force us to reconsider our definitions of `best' and of rationality.
Ideally, an AutoML system would be bounded rational\fTBD{cite}.
This is likely very difficult to do, but reinforcement learning techniques could be used to get closer to an asymptotically bounded rational system (maybe).

%\newpage

\small

\bibliography{library}
\bibliographystyle{unsrt}

\end{document} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%